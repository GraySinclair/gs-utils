{"cells":[{"cell_type":"markdown","source":["In Fabric, a Python (Spark) notebook talks to the Spark engine via Livy. Every time you submit a job or command to Spark (e.g. `df.count()`, `df.show()`, `spark.read.table(...)`, `df.write...`), you send a request through Livy.\n","\n","The main ideas here:\n","- Raw Python will execute row by row(eager execution), while Spark will create a plan using your instructions then execute(lazy execution). \n","    - This means a tight python for-loop that iterates row-by-row for 2 spark changes can easily generate 200 livy calls on a small 100-row table.\n","- Many small Spark actions in Python loops = many Livy calls = easier to hit rate limits.\n","- Fewer, larger actions using spark syntax `withColumn`, joins, and unions means fewer Livy calls. "],"metadata":{},"id":"057db497"},{"cell_type":"markdown","source":["## Sample data"],"metadata":{},"id":"2f5a3aa4"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","data = [\n","    (1, 'A', 10.0),\n","    (2, 'B', 20.0),\n","    (3, 'C', 30.0),\n","    (4, 'D', 40.0),\n","]\n","df = spark.createDataFrame(data, ['id', 'category', 'value'])\n","df.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"ed84ed17-aab4-461a-9b62-3705960acbbf","normalized_state":"finished","queued_time":"2025-11-19T16:31:57.6077205Z","session_start_time":"2025-11-19T16:31:57.6088091Z","execution_start_time":"2025-11-19T16:32:10.0640053Z","execution_finish_time":"2025-11-19T16:32:13.9341982Z","parent_msg_id":"e5f97f7f-5904-4b96-9f91-20541c868cec"},"text/plain":"StatementMeta(, ed84ed17-aab4-461a-9b62-3705960acbbf, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+---+--------+-----+\n| id|category|value|\n+---+--------+-----+\n|  1|       A| 10.0|\n|  2|       B| 20.0|\n|  3|       C| 30.0|\n|  4|       D| 40.0|\n+---+--------+-----+\n\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6ea15af7"},{"cell_type":"markdown","source":["## Per‑row spark work inside a python loop (chatty = bad)\n","\n","This is the pattern that most easily triggers Livy limits.\n","\n","Characteristics:\n","- A `for` loop in Python.\n","- Inside the loop you call Spark actions (`count`, `show`, `collect`, `write`, `spark.read...`).\n","- Each iteration submits a new Spark job  a new Livy call.\n","\n","In Fabric, doing this for hundreds or thousands of rows / tables can quickly hit gateway limits."],"metadata":{},"id":"08847f82"},{"cell_type":"code","source":["# DO NOT USE THIS PATTERN IN PRODUCTION\n","from time import sleep\n","\n","ids = [r.id for r in df.select('id').collect()]  # one Spark job here\n","\n","for i in ids:\n","    # Each of these is a separate Spark job (and a Livy request in Fabric)\n","    subset = df.filter(F.col('id') == i)   # job when triggered by an action\n","    cnt = subset.count()                   # Spark action = Livy call\n","    print(f'id={i}, count={cnt}')\n","    sleep(0.1) # simulate some work\n","\n","print('Finished chatty pe row loop')"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3db22290"},{"cell_type":"markdown","source":["## Single collect + pure python loop (no extra Livy calls, but not scalable)\n","\n","Here we **only call Spark once** (`collect()`), then do everything else in pure Python. This does **not** introduce extra Livy calls, but it has two downsides:\n","- All data must fit in the driver memory.\n","- You lose the benefits of distributed execution.\n","\n","Use this only for small data or debugging."],"metadata":{},"id":"2c8581aa"},{"cell_type":"code","source":["rows = df.collect()  # single Spark action -> one Livy call\n","\n","total = 0.0\n","for row in rows:\n","    # Pure Python logic, no Spark actions here but loses the distributed execution (not really scalable)\n","    if row.category in ('A', 'B', 'C'):\n","        total += row.value * 1.1\n","\n","print(f'Total (pure Python after collect): {total}')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"ed84ed17-aab4-461a-9b62-3705960acbbf","normalized_state":"finished","queued_time":"2025-11-19T16:32:24.8708832Z","session_start_time":null,"execution_start_time":"2025-11-19T16:32:24.8721023Z","execution_finish_time":"2025-11-19T16:32:26.074179Z","parent_msg_id":"68bf2c29-9418-49d8-80b7-908e070049bf"},"text/plain":"StatementMeta(, ed84ed17-aab4-461a-9b62-3705960acbbf, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Total (pure Python after collect): 66.0\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7a03b5c4"},{"cell_type":"markdown","source":["## lazy ops(vectorized) + a small number of actions (preferred)\n","\n","Instead of looping row‑by‑row in Python, push the logic into Spark using expressions or User Defined Functions(UDFs are items just like notebooks when you get a chance to explore) The key is:\n","- Build a transformation plan with `withColumn`, `filter`, `groupBy`, `agg`, etc. (lazy operations).\n","- Trigger the work with a small number of actions (e.g. one `write`, one `count`).\n","\n","This pattern keeps Livy calls to a minimum and lets Spark optimize the execution plan."],"metadata":{},"id":"fa46e10b"},{"cell_type":"code","source":["# Example: compute an adjusted_value using an expression\n","df_transformed = (df.withColumn('adjusted_value', \n","                                F.when(F.col('category').isin('A', 'B', 'C'), F.col('value') * 1.1)\n","                                    .otherwise(F.col('value'))\n","    )\n",")\n","\n","df_transformed.show() # One action: show or write – one Livy call"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"ed84ed17-aab4-461a-9b62-3705960acbbf","normalized_state":"finished","queued_time":"2025-11-19T16:33:48.8419811Z","session_start_time":null,"execution_start_time":"2025-11-19T16:33:48.8432348Z","execution_finish_time":"2025-11-19T16:33:49.8505806Z","parent_msg_id":"af426208-ea72-463e-87a5-b0c6b7b5f048"},"text/plain":"StatementMeta(, ed84ed17-aab4-461a-9b62-3705960acbbf, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+---+--------+-----+--------------+\n| id|category|value|adjusted_value|\n+---+--------+-----+--------------+\n|  1|       A| 10.0|          11.0|\n|  2|       B| 20.0|          22.0|\n|  3|       C| 30.0|          33.0|\n|  4|       D| 40.0|          40.0|\n+---+--------+-----+--------------+\n\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9f8f8302"},{"cell_type":"markdown","source":["## Many small actions vs a single action\n","\n","Even without explicit `for` loops, **many small actions** can be chatty:\n","\n","```python\n","df.filter(...).count()\n","df.filter(...).count()\n","df.filter(...).count()\n","```\n","\n","Each `count()` is a separate job and a separate Livy request. Contrast that with a more compact approach that aggregates once."],"metadata":{},"id":"0f7a916b"},{"cell_type":"code","source":["# Counts by category (chatty pattern)\n","for cat in ['A', 'B', 'C', 'D']:\n","    cnt = df.filter(F.col('category') == cat).count()  # separate job per iteration\n","    print(cat, cnt)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0242a562"},{"cell_type":"code","source":["# Counts by category (single aggregation)\n","df.groupBy('category').count().show() # one agg -> one livy job"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"ed84ed17-aab4-461a-9b62-3705960acbbf","normalized_state":"finished","queued_time":"2025-11-19T16:34:22.6457269Z","session_start_time":null,"execution_start_time":"2025-11-19T16:34:22.6469193Z","execution_finish_time":"2025-11-19T16:34:24.409712Z","parent_msg_id":"7ba2be86-d39d-4e79-a612-2acf9ca79f91"},"text/plain":"StatementMeta(, ed84ed17-aab4-461a-9b62-3705960acbbf, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+--------+-----+\n|category|count|\n+--------+-----+\n|       A|    1|\n|       B|    1|\n|       C|    1|\n|       D|    1|\n+--------+-----+\n\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b4d6b6a7"}],"metadata":{"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"kernel_info":{"name":"synapse_pyspark"}},"nbformat":4,"nbformat_minor":5}